{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b1df98-b5eb-4f3d-97ee-621c822cd3ec",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee86b2e-87d1-49a7-bb93-de6cb4ff6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "PROJECT_ID = \"jchavezar-demo\"\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "UUID = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "BUCKET_NAME = \"vtx-demos\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "DATA_PATH = \"data\"\n",
    "# Base\n",
    "DATASET_NAME = \"news_corpora\"\n",
    "DATASET_URI = f\"{BUCKET_URI}/{DATA_PATH}/raw/newsCorpora.csv\"\n",
    "\n",
    "# Experiments\n",
    "TASK = \"classification\"\n",
    "MODEL_TYPE = \"naivebayes\"\n",
    "EXPERIMENT_NAME = f\"{TASK}-{MODEL_TYPE}-{UUID}\"\n",
    "EXPERIMENT_RUN_NAME = \"run-1\"\n",
    "\n",
    "# Preprocessing\n",
    "PREPROCESSED_DATASET_NAME = f\"preprocessed_{DATASET_NAME}\"\n",
    "PREPROCESS_EXECUTION_NAME = \"preprocess\"\n",
    "COLUMN_NAMES = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"url\",\n",
    "    \"publisher\",\n",
    "    \"category\",\n",
    "    \"story\",\n",
    "    \"hostname\",\n",
    "    \"timestamp\",\n",
    "]\n",
    "DELIMITER = \"\t\"\n",
    "INDEX_COL = 0\n",
    "PREPROCESSED_DATASET_URI = (\n",
    "    f\"{BUCKET_URI}/{DATA_PATH}/preprocess/{PREPROCESSED_DATASET_NAME}.csv\"\n",
    ")\n",
    "\n",
    "# Training\n",
    "TRAIN_EXECUTION_NAME = \"train\"\n",
    "TARGET = \"category\"\n",
    "FEATURES = \"title\"\n",
    "TEST_SIZE = 0.2\n",
    "SEED = 8\n",
    "TRAINED_MODEL_URI = f\"{BUCKET_URI}/deliverables/{UUID}\"\n",
    "MODEL_NAME = f\"{EXPERIMENT_NAME}-model\"\n",
    "SERVE_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3c86d-b124-4226-9f51-a3383126bfca",
   "metadata": {},
   "source": [
    "### Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f85dd9-3f45-4330-ac75-047e3d0cd3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-27 15:35:40--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29224203 (28M) [application/x-httpd-php]\n",
      "Saving to: ‘data/NewsAggregatorDataset.zip’\n",
      "\n",
      "NewsAggregatorDatas 100%[===================>]  27.87M  34.2MB/s    in 0.8s    \n",
      "\n",
      "2022-09-27 15:35:41 (34.2 MB/s) - ‘data/NewsAggregatorDataset.zip’ saved [29224203/29224203]\n",
      "\n",
      "Archive:  data/NewsAggregatorDataset.zip\n",
      "  inflating: data/temp/2pageSessions.csv  \n",
      "   creating: data/temp/__MACOSX/\n",
      "  inflating: data/temp/__MACOSX/._2pageSessions.csv  \n",
      "  inflating: data/temp/newsCorpora.csv  \n",
      "  inflating: data/temp/__MACOSX/._newsCorpora.csv  \n",
      "  inflating: data/temp/readme.txt    \n",
      "  inflating: data/temp/__MACOSX/._readme.txt  \n",
      "Copying file://data/raw/newsCorpora.csv [Content-Type=text/csv]...\n",
      "Copying file://data/raw/2pageSessions.csv [Content-Type=text/csv]...            \n",
      "\\ [2/2 files][100.5 MiB/100.5 MiB] 100% Done                                    \n",
      "Operation completed over 2 objects/100.5 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!mkdir -m 777 -p {DATA_PATH}\n",
    "\n",
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\"\n",
    "!wget --no-parent {DATASET_URL} --directory-prefix={DATA_PATH}\n",
    "!mkdir -m 777 -p {DATA_PATH}/temp {DATA_PATH}/raw\n",
    "!unzip {DATA_PATH}/*.zip -d {DATA_PATH}/temp\n",
    "!mv {DATA_PATH}/temp/*.csv {DATA_PATH}/raw && rm -Rf {DATA_PATH}/temp && rm -f {DATA_PATH}/*.zip\n",
    "!gsutil -m cp -R {DATA_PATH}/raw $BUCKET_URI/{DATA_PATH}/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14fa1f7-03ae-48ec-84ea-d06a627bd3a2",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6f56d-b912-41e1-bba9-3a54f9d75c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import logging\n",
    "import collections\n",
    "import tempfile\n",
    "import time\n",
    "from json import dumps\n",
    "\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73aa18b-e30c-422a-a47e-bd86fa85fabf",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1509c-15f2-4033-8dd0-e66a33b6036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID, experiment=EXPERIMENT_NAME, staging_bucket=BUCKET_URI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726fd85-7e93-45fa-8c05-bd65ac495ac1",
   "metadata": {},
   "source": [
    "### Initialize Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616ad3c-81b8-4db6-a5d9-a0e6ca720c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/569083142710/locations/us-central1/metadataStores/default/contexts/classification-naivebayes-kkafjjcq-run-1 to Experiment: classification-naivebayes-kkafjjcq\n"
     ]
    }
   ],
   "source": [
    "run = vertex_ai.start_run(EXPERIMENT_RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1c38a-84a7-4567-be74-3e58f0c1cb6f",
   "metadata": {},
   "source": [
    "### Create a Dataset Metadata Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "facf21b2-6920-4590-8347-3977ae0125be",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_artifact = vertex_ai.Artifact.create(\n",
    "    schema_title=\"system.Dataset\", display_name=DATASET_NAME, uri=DATASET_URI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb3e74-2e3b-479d-ac51-e64b6853a0fc",
   "metadata": {},
   "source": [
    "### Create preprocess module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6adc2ce1-8ef1-44ce-b6bf-80d74498b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocess module\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame, text_col: str) -> pd.DataFrame:\n",
    "    preprocessed_df = df.copy()\n",
    "    preprocessed_df[text_col] = preprocessed_df[text_col].apply(lambda x: x.lower())\n",
    "    preprocessed_df[text_col] = preprocessed_df[text_col].apply(\n",
    "        lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    )\n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2510c-06bb-40c0-9e96-bf79a30ee372",
   "metadata": {},
   "source": [
    "### Add preprocess execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843892ec-7f79-436f-84d4-d3515c8cb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with vertex_ai.start_execution(\n",
    "    schema_title=\"system.ContainerExecution\", display_name=PREPROCESS_EXECUTION_NAME\n",
    ") as exc:\n",
    "    logging.info(f\"Start {PREPROCESS_EXECUTION_NAME} execution.\")\n",
    "    exc.assign_input_artifacts([raw_dataset_artifact])\n",
    "\n",
    "    # Log preprocessing params --------------------------------------------------\n",
    "    logging.info(\"Log preprocessing params.\")\n",
    "    vertex_ai.log_params(\n",
    "        {\n",
    "            \"delimiter\": DELIMITER,\n",
    "            \"features\": dumps(COLUMN_NAMES),\n",
    "            \"index_col\": INDEX_COL,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Preprocessing ------------------------------------------------------------\n",
    "    logging.info(\"Preprocessing.\")\n",
    "    raw_df = pd.read_csv(\n",
    "        raw_dataset_artifact.uri,\n",
    "        delimiter=DELIMITER,\n",
    "        names=COLUMN_NAMES,\n",
    "        index_col=INDEX_COL,\n",
    "    )\n",
    "    preprocessed_df = preprocess(raw_df, \"title\")\n",
    "    preprocessed_df.to_csv(PREPROCESSED_DATASET_URI, sep=\",\")\n",
    "\n",
    "    # Log preprocessing metrics and store dataset artifact ---------------------\n",
    "    logging.info(f\"Log preprocessing metrics and {PREPROCESSED_DATASET_NAME} dataset.\")\n",
    "    vertex_ai.log_metrics(\n",
    "        {\n",
    "            \"n_records\": preprocessed_df.shape[0],\n",
    "            \"n_columns\": preprocessed_df.shape[1],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    preprocessed_dataset_metadata = vertex_ai.Artifact.create(\n",
    "        schema_title=\"system.Dataset\",\n",
    "        display_name=PREPROCESSED_DATASET_NAME,\n",
    "        uri=PREPROCESSED_DATASET_URI,\n",
    "    )\n",
    "    exc.assign_output_artifacts([preprocessed_dataset_metadata])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60479159-3490-4fd4-b19a-b5a938f47d69",
   "metadata": {},
   "source": [
    "### Create Model Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5af1fd8-0353-4f29-8d48-3e7d1fb26e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train module\n",
    "\"\"\"\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def get_training_split(\n",
    "    x: pd.DataFrame, y: pd.Series, test_size: float, random_state: int\n",
    ") -> (pd.DataFrame, pd.Series, pd.DataFrame, pd.Series):\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n",
    "\n",
    "def get_pipeline():\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            (\"vect\", CountVectorizer()),\n",
    "            (\"tfidf\", TfidfTransformer()),\n",
    "            (\"clf\", MultinomialNB()),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_pipeline(model: Pipeline, X_train: pd.Series, y_train: pd.Series) -> Pipeline:\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model: Pipeline, X_test: pd.Series, y_test: pd.Series) -> float:\n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Store evaluation metrics\n",
    "    # Store evaluation metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": round(accuracy_score(y_test, y_pred), 5),\n",
    "        \"precision\": round(precision_score(y_test, y_pred, average=\"weighted\"), 5),\n",
    "        \"recall\": round(recall_score(y_test, y_pred, average=\"weighted\"), 5),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_model(model: Pipeline, save_path: str) -> int:\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile() as tmp:\n",
    "            joblib.dump(trained_pipeline, filename=tmp.name)\n",
    "            ! gsutil cp {tmp.name} {save_path}/model.joblib\n",
    "    except RuntimeError as error:\n",
    "        print(error)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61528d-1147-476e-9586-d63e623fa54c",
   "metadata": {},
   "source": [
    "### Add the train execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16466f02-55f3-405a-a018-21f6006995e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vertex_ai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/1409519146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m with vertex_ai.start_execution(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mschema_title\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"system.ContainerExecution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_EXECUTION_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m ) as exc:\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_input_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreprocessed_dataset_metadata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vertex_ai' is not defined"
     ]
    }
   ],
   "source": [
    "with vertex_ai.start_execution(\n",
    "    schema_title=\"system.ContainerExecution\", display_name=TRAIN_EXECUTION_NAME\n",
    ") as exc:\n",
    "\n",
    "    exc.assign_input_artifacts([preprocessed_dataset_metadata])\n",
    "\n",
    "    # Get training and testing data\n",
    "    logging.info(\"Get training and testing data.\")\n",
    "    x_train, x_val, y_train, y_val = get_training_split(\n",
    "        preprocessed_df[FEATURES],\n",
    "        preprocessed_df[TARGET],\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    # Get model pipeline\n",
    "    logging.info(\"Get model pipeline.\")\n",
    "    pipeline = get_pipeline()\n",
    "\n",
    "    # Log training param -------------------------------------------------------\n",
    "\n",
    "    # Log data parameters\n",
    "    logging.info(\"Log data parameters.\")\n",
    "    vertex_ai.log_params(\n",
    "        {\n",
    "            \"target\": TARGET,\n",
    "            \"features\": FEATURES,\n",
    "            \"test_size\": TEST_SIZE,\n",
    "            \"random_state\": SEED,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log pipeline parameters\n",
    "    logging.info(\"Log pipeline parameters.\")\n",
    "    vertex_ai.log_params(\n",
    "        {\n",
    "            \"pipeline_steps\": dumps(\n",
    "                {step[0]: str(step[1].__class__.__name__) for step in pipeline.steps}\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Training -----------------------------------------------------------------\n",
    "\n",
    "    # Train model pipeline\n",
    "    logging.info(\"Train model pipeline.\")\n",
    "    train_start = time.time()\n",
    "    trained_pipeline = train_pipeline(pipeline, x_train, y_train)\n",
    "    train_end = time.time()\n",
    "\n",
    "    # Evaluate model\n",
    "    logging.info(\"Evaluate model.\")\n",
    "    model_metrics = evaluate_model(trained_pipeline, x_val, y_val)\n",
    "\n",
    "    # Log training metrics and store model artifact ----------------------------\n",
    "\n",
    "    # Log training metrics\n",
    "    logging.info(\"Log training metrics.\")\n",
    "    vertex_ai.log_metrics(model_metrics)\n",
    "\n",
    "    # Generate first ten predictions\n",
    "    logging.info(\"Generate prediction sample.\")\n",
    "    prediction_sample = trained_pipeline.predict(x_val)[:10]\n",
    "    print(\"prediction sample:\", prediction_sample)\n",
    "\n",
    "    # Upload Model on Vertex AI\n",
    "    logging.info(\"Upload Model on Vertex AI.\")\n",
    "    loaded = save_model(trained_pipeline, TRAINED_MODEL_URI)\n",
    "    if loaded:\n",
    "        model = vertex_ai.Model.upload(\n",
    "            serving_container_image_uri=SERVE_IMAGE,\n",
    "            artifact_uri=TRAINED_MODEL_URI,\n",
    "            display_name=MODEL_NAME,\n",
    "        )\n",
    "\n",
    "    exc.assign_output_artifacts([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a54eb-349a-4b78-af12-9f309316a5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
